{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "challenging-capability",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:04:56.556773Z",
     "iopub.status.busy": "2021-06-14T01:04:56.556125Z",
     "iopub.status.idle": "2021-06-14T01:05:02.380156Z",
     "shell.execute_reply": "2021-06-14T01:05:02.380900Z",
     "shell.execute_reply.started": "2021-06-14T00:56:22.728433Z"
    },
    "papermill": {
     "duration": 5.857888,
     "end_time": "2021-06-14T01:05:02.381259",
     "exception": false,
     "start_time": "2021-06-14T01:04:56.523371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import h5py\n",
    "import time\n",
    "from PIL import Image\n",
    "import requests\n",
    "import imageio\n",
    "from io import BytesIO\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vital-missile",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:05:02.421988Z",
     "iopub.status.busy": "2021-06-14T01:05:02.421401Z",
     "iopub.status.idle": "2021-06-14T01:05:12.015071Z",
     "shell.execute_reply": "2021-06-14T01:05:12.014288Z",
     "shell.execute_reply.started": "2021-06-14T00:56:28.693862Z"
    },
    "papermill": {
     "duration": 9.614957,
     "end_time": "2021-06-14T01:05:12.015251",
     "exception": false,
     "start_time": "2021-06-14T01:05:02.400294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Required images are saved as an hdf5 file and uploaded to Kaggle as a Dataset.  \n",
    "# Below saves images from the dataset to a numpy array.\n",
    "hf = h5py.File(\"../input/images/images.hdf5\", \"r\")\n",
    "images = np.array(hf[\"/images\"]).astype(\"uint8\")\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "natural-contribution",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:05:12.076891Z",
     "iopub.status.busy": "2021-06-14T01:05:12.076181Z",
     "iopub.status.idle": "2021-06-14T01:05:12.235979Z",
     "shell.execute_reply": "2021-06-14T01:05:12.236453Z",
     "shell.execute_reply.started": "2021-06-14T00:56:37.830128Z"
    },
    "papermill": {
     "duration": 0.20213,
     "end_time": "2021-06-14T01:05:12.236655",
     "exception": false,
     "start_time": "2021-06-14T01:05:12.034525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f949af2b650>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(images[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "finished-material",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:05:12.283692Z",
     "iopub.status.busy": "2021-06-14T01:05:12.282999Z",
     "iopub.status.idle": "2021-06-14T01:05:12.613706Z",
     "shell.execute_reply": "2021-06-14T01:05:12.614293Z",
     "shell.execute_reply.started": "2021-06-14T00:56:38.023922Z"
    },
    "papermill": {
     "duration": 0.357009,
     "end_time": "2021-06-14T01:05:12.614520",
     "exception": false,
     "start_time": "2021-06-14T01:05:12.257511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images before removal: 65268\n",
      "Number of images after removal: 51371\n"
     ]
    }
   ],
   "source": [
    "# I have manually inspected all of the images and saved the indexes I wanted to remove in a file.\n",
    "# This code block imports those indexes and remove them from the images array.\n",
    "file1 = open('../input/removeindexes/1removeIndexes.txt', 'r')\n",
    "indexes = file1.readlines()\n",
    "indexes = list(map(int, indexes))\n",
    "print(f'Number of images before removal: {len(images)}')\n",
    "images = np.delete(images, indexes, axis = 0)\n",
    "print(f'Number of images after removal: {len(images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adult-vault",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:05:12.665479Z",
     "iopub.status.busy": "2021-06-14T01:05:12.664663Z",
     "iopub.status.idle": "2021-06-14T01:05:12.739591Z",
     "shell.execute_reply": "2021-06-14T01:05:12.738942Z",
     "shell.execute_reply.started": "2021-06-14T00:56:38.242577Z"
    },
    "papermill": {
     "duration": 0.104416,
     "end_time": "2021-06-14T01:05:12.739766",
     "exception": false,
     "start_time": "2021-06-14T01:05:12.635350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Conv2DTranspose, LeakyReLU, Reshape, BatchNormalization, Activation, Conv2D, Flatten, Dropout, AveragePooling2D, UpSampling2D, Input, Lambda, GaussianNoise\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.initializers import RandomNormal\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from keras import backend\n",
    "from keras.constraints import Constraint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "driving-princess",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:05:12.789650Z",
     "iopub.status.busy": "2021-06-14T01:05:12.787871Z",
     "iopub.status.idle": "2021-06-14T01:05:15.958876Z",
     "shell.execute_reply": "2021-06-14T01:05:15.959409Z",
     "shell.execute_reply.started": "2021-06-14T00:56:38.332889Z"
    },
    "papermill": {
     "duration": 3.19887,
     "end_time": "2021-06-14T01:05:15.959634",
     "exception": false,
     "start_time": "2021-06-14T01:05:12.760764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_images(images):\n",
    "\t# convert from unit8 to float32\n",
    "\timages = images.astype('float32')\n",
    "\t# scale from [0,255] to [-1,1]\n",
    "\timages = (images - 127.5) / 127.5\n",
    "\treturn images\n",
    "\n",
    "images = scale_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "worth-excellence",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:05:16.065762Z",
     "iopub.status.busy": "2021-06-14T01:05:16.013069Z",
     "iopub.status.idle": "2021-06-14T01:05:21.512091Z",
     "shell.execute_reply": "2021-06-14T01:05:21.512619Z",
     "shell.execute_reply.started": "2021-06-14T00:56:40.849620Z"
    },
    "papermill": {
     "duration": 5.532111,
     "end_time": "2021-06-14T01:05:21.512848",
     "exception": false,
     "start_time": "2021-06-14T01:05:15.980737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "imposed-gallery",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:05:21.571054Z",
     "iopub.status.busy": "2021-06-14T01:05:21.570307Z",
     "iopub.status.idle": "2021-06-14T01:05:21.573650Z",
     "shell.execute_reply": "2021-06-14T01:05:21.573079Z",
     "shell.execute_reply.started": "2021-06-14T00:59:47.108507Z"
    },
    "papermill": {
     "duration": 0.038965,
     "end_time": "2021-06-14T01:05:21.573811",
     "exception": false,
     "start_time": "2021-06-14T01:05:21.534846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definining the generator model. Commented out codelines are alternative layers that could be used.\n",
    "def image_generator():\n",
    "\n",
    "    generator = Sequential()\n",
    "\n",
    "    generator.add(Dense(256*4*4, input_shape = (100,)))\n",
    "    #generator.add(Dense(512*4*4, input_shape = (100,)))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU())\n",
    "    generator.add(Reshape((4,4,256)))\n",
    "    #generator.add(Reshape((8,8,512)))\n",
    "\n",
    "    init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    generator.add(UpSampling2D())\n",
    "    #generator.add(tf.keras.layers.Lambda(lambda x: tf.nn.depth_to_space(x, 2)))\n",
    "    #generator.add(Conv2DTranspose(128,kernel_size=2, strides=2, padding = \"same\", kernel_initializer=init))\n",
    "\n",
    "    generator.add(Conv2D(128, kernel_size=4, padding = \"same\", kernel_initializer=init))\n",
    "    generator.add(BatchNormalization())\n",
    "    #generator.add(Dropout(0.2))\n",
    "    generator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    generator.add(Conv2D(128, kernel_size=4, padding = \"same\", kernel_initializer=init))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    generator.add(UpSampling2D())\n",
    "    #generator.add(tf.keras.layers.Lambda(lambda x: tf.nn.depth_to_space(x, 2)))\n",
    "    #generator.add(Conv2DTranspose(128,kernel_size=4, strides=2, padding = \"same\", kernel_initializer=init))\n",
    "\n",
    "    generator.add(Conv2D(128, kernel_size=4, padding = \"same\", kernel_initializer=init))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "\n",
    "    ##generator.add(Conv2DTranspose(128,kernel_size=4, strides=2, padding = \"same\", kernel_initializer=init))\n",
    "    generator.add(UpSampling2D())\n",
    "    #generator.add(tf.keras.layers.Lambda(lambda x: tf.nn.depth_to_space(x, 2)))\n",
    "\n",
    "    generator.add(Conv2D(128, kernel_size=4, padding = \"same\", kernel_initializer=init))\n",
    "    generator.add(BatchNormalization())\n",
    "    #generator.add(Dropout(0.2))\n",
    "    generator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "\n",
    "    ##generator.add(Conv2DTranspose(128,kernel_size=4, strides=2, padding = \"same\", kernel_initializer=init))\n",
    "    generator.add(UpSampling2D())\n",
    "    #generator.add(tf.keras.layers.Lambda(lambda x: tf.nn.depth_to_space(x, 2)))\n",
    "\n",
    "    generator.add(Conv2D(128, kernel_size=4, padding = \"same\", kernel_initializer=init))\n",
    "    #generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    generator.add(Conv2D(3,kernel_size=4, padding = \"same\", activation='tanh', kernel_initializer=init))\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pregnant-daniel",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:05:21.623775Z",
     "iopub.status.busy": "2021-06-14T01:05:21.623116Z",
     "iopub.status.idle": "2021-06-14T01:05:23.608323Z",
     "shell.execute_reply": "2021-06-14T01:05:23.608819Z",
     "shell.execute_reply.started": "2021-06-14T00:59:49.995532Z"
    },
    "papermill": {
     "duration": 2.013102,
     "end_time": "2021-06-14T01:05:23.609010",
     "exception": false,
     "start_time": "2021-06-14T01:05:21.595908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 4096)              413696    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 8, 8, 128)         524416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 128)         262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 64, 3)         6147      \n",
      "=================================================================\n",
      "Total params: 2,011,779\n",
      "Trainable params: 2,002,563\n",
      "Non-trainable params: 9,216\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# For GPU Training\\ngenerator_model = image_generator()\\ngenerator_model.summary()\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# For TPU Training\n",
    "with strategy.scope():\n",
    "    generator_model = image_generator()\n",
    "    generator_model.summary()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# For GPU Training\n",
    "generator_model = image_generator()\n",
    "generator_model.summary()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "addressed-norway",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:05:23.662397Z",
     "iopub.status.busy": "2021-06-14T01:05:23.661637Z",
     "iopub.status.idle": "2021-06-14T01:05:23.665300Z",
     "shell.execute_reply": "2021-06-14T01:05:23.665774Z",
     "shell.execute_reply.started": "2021-06-14T00:56:48.322681Z"
    },
    "papermill": {
     "duration": 0.034516,
     "end_time": "2021-06-14T01:05:23.665977",
     "exception": false,
     "start_time": "2021-06-14T01:05:23.631461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# implementation of wasserstein loss\\ndef wasserstein_loss(y_true, y_pred):\\n    return backend.mean(y_true * y_pred)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_input_data(n_samples):\n",
    "  X = np.random.randn(100 * n_samples)\n",
    "  X = X.reshape(n_samples, 100)\n",
    "  return X\n",
    "\n",
    "def create_fake_data(generator_model, n_samples):\n",
    "  input = generate_input_data(n_samples)\n",
    "  X = generator_model.predict(input)\n",
    "  #y = np.zeros((n_samples, 1))\n",
    "  y = np.ones((n_samples, 1))\n",
    "  return X,y\n",
    "\n",
    "'''\n",
    "# implementation of wasserstein loss\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return backend.mean(y_true * y_pred)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unsigned-malaysia",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:05:23.720574Z",
     "iopub.status.busy": "2021-06-14T01:05:23.719766Z",
     "iopub.status.idle": "2021-06-14T01:05:23.735307Z",
     "shell.execute_reply": "2021-06-14T01:05:23.734712Z",
     "shell.execute_reply.started": "2021-06-14T00:59:55.138989Z"
    },
    "papermill": {
     "duration": 0.046609,
     "end_time": "2021-06-14T01:05:23.735475",
     "exception": false,
     "start_time": "2021-06-14T01:05:23.688866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definining the discriminator model. Commented out codelines are alternative layers that could be used.\n",
    "def image_discriminator():\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Conv2D(128, kernel_size=4, padding = \"same\", input_shape = (64,64,3), kernel_initializer=init))\n",
    "    discriminator.add(LeakyReLU(alpha=0.2))\n",
    "    #discriminator.add(Dropout(0.4))\n",
    "    \n",
    "\n",
    "    discriminator.add(Conv2D(128, kernel_size=4,strides=(2,2), padding = \"same\", kernel_initializer=init))\n",
    "    #discriminator.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    #discriminator.add(BatchNormalization())\n",
    "    #discriminator.add(GaussianNoise(0.2))\n",
    "    discriminator.add(LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(Dropout(0.4))\n",
    "    \n",
    "\n",
    "    discriminator.add(Conv2D(128, kernel_size=4,strides=(2,2), padding = \"same\", kernel_initializer=init))\n",
    "    #discriminator.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    #discriminator.add(BatchNormalization())\n",
    "    #discriminator.add(GaussianNoise(0.2))\n",
    "    discriminator.add(LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "    discriminator.add(Conv2D(128, kernel_size=4,strides=(2,2), padding = \"same\", kernel_initializer=init))\n",
    "    #discriminator.add(BatchNormalization())\n",
    "    #discriminator.add(GaussianNoise(0.2))\n",
    "    discriminator.add(LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(Dropout(0.4))\n",
    "    \n",
    "\n",
    "    discriminator.add(Conv2D(128, kernel_size=4,strides=(2,2), padding = \"same\", kernel_initializer=init))\n",
    "    #discriminator.add(BatchNormalization())\n",
    "    #discriminator.add(GaussianNoise(0.2))\n",
    "    discriminator.add(LeakyReLU(alpha=0.2))\n",
    "    #discriminator.add(Dropout(0.2))\n",
    "\n",
    "    discriminator.add(Flatten())\n",
    "    #discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    discriminator.add(Dense(1, activation='linear'))\n",
    "\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "impressive-buddy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:05:23.789352Z",
     "iopub.status.busy": "2021-06-14T01:05:23.788427Z",
     "iopub.status.idle": "2021-06-14T01:05:24.247770Z",
     "shell.execute_reply": "2021-06-14T01:05:24.248406Z",
     "shell.execute_reply.started": "2021-06-14T00:59:57.944131Z"
    },
    "papermill": {
     "duration": 0.49023,
     "end_time": "2021-06-14T01:05:24.248602",
     "exception": false,
     "start_time": "2021-06-14T01:05:23.758372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 128)       6272      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 64, 64, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 4, 4, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 2,106,497\n",
      "Trainable params: 2,106,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndiscriminator_model = image_discriminator()\\ndiscriminator_model.summary()\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# For TPU Training\n",
    "with strategy.scope():\n",
    "    discriminator_model = image_discriminator()\n",
    "    discriminator_model.summary()\n",
    "\n",
    "# For GPU Training\n",
    "\"\"\"\n",
    "discriminator_model = image_discriminator()\n",
    "discriminator_model.summary()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "peaceful-roman",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:05:24.302918Z",
     "iopub.status.busy": "2021-06-14T01:05:24.302160Z",
     "iopub.status.idle": "2021-06-14T01:05:24.306182Z",
     "shell.execute_reply": "2021-06-14T01:05:24.306696Z",
     "shell.execute_reply.started": "2021-06-14T00:56:48.764619Z"
    },
    "papermill": {
     "duration": 0.034908,
     "end_time": "2021-06-14T01:05:24.306883",
     "exception": false,
     "start_time": "2021-06-14T01:05:24.271975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51371, 64, 64, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fiscal-adjustment",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:05:24.365920Z",
     "iopub.status.busy": "2021-06-14T01:05:24.365130Z",
     "iopub.status.idle": "2021-06-14T01:05:24.368111Z",
     "shell.execute_reply": "2021-06-14T01:05:24.367568Z",
     "shell.execute_reply.started": "2021-06-14T01:00:07.960247Z"
    },
    "papermill": {
     "duration": 0.037124,
     "end_time": "2021-06-14T01:05:24.368280",
     "exception": false,
     "start_time": "2021-06-14T01:05:24.331156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from numpy.random import choice\n",
    "\n",
    "def load_real_data(dataset, n_samples):\n",
    "  ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "  X = dataset[ix]\n",
    "  y = -np.ones((n_samples, 1))\n",
    "  return X,y\n",
    "\n",
    "def load_fake_data(n_samples):\n",
    "  X = np.random.rand(64 * 64 * 3 * n_samples)\n",
    "  X = -1 + X * 2\n",
    "  X = X.reshape((n_samples, 64,64,3))\n",
    "  y = np.ones((n_samples, 1))\n",
    "  return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "professional-parish",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:05:24.427909Z",
     "iopub.status.busy": "2021-06-14T01:05:24.426783Z",
     "iopub.status.idle": "2021-06-14T01:05:24.444440Z",
     "shell.execute_reply": "2021-06-14T01:05:24.443802Z",
     "shell.execute_reply.started": "2021-06-14T01:00:10.741624Z"
    },
    "papermill": {
     "duration": 0.051951,
     "end_time": "2021-06-14T01:05:24.444600",
     "exception": false,
     "start_time": "2021-06-14T01:05:24.392649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resource: https://keras.io/examples/generative/wgan_gp/#create-the-wgangp-model\n",
    "class WGAN(keras.Model):\n",
    "    def __init__(\n",
    "            self,\n",
    "            discriminator,\n",
    "            generator,\n",
    "            latent_dim,\n",
    "            discriminator_extra_steps=5,\n",
    "            gp_weight=10.0,\n",
    "        ):\n",
    "            super(WGAN, self).__init__()\n",
    "            self.discriminator = discriminator\n",
    "            self.generator = generator\n",
    "            self.latent_dim = latent_dim\n",
    "            self.d_steps = discriminator_extra_steps\n",
    "            self.gp_weight = gp_weight\n",
    "    \n",
    "    # For TPU Training\n",
    "    \n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn, steps_per_execution):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "        self.steps_per_execution = steps_per_execution\n",
    "    \n",
    "    # For GPU Training\n",
    "    \"\"\"\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "    \"\"\"\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        # For WGAN-GP\n",
    "        #gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        # For WGAN-LP\n",
    "        gp = tf.reduce_mean((tf.maximum(0., norm - 1.0)) ** 2)\n",
    "        return gp\n",
    "        \n",
    "    def train_step(self, real_images):\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images from the latent vector\n",
    "                fake_images = self.generator(random_latent_vectors, training=True)\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator(fake_images, training=True)\n",
    "                # Get the logits for the real images\n",
    "                real_logits = self.discriminator(real_images, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = self.generator(random_latent_vectors, training=True)\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "noble-demonstration",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:05:24.560729Z",
     "iopub.status.busy": "2021-06-14T01:05:24.560008Z",
     "iopub.status.idle": "2021-06-14T01:05:24.564790Z",
     "shell.execute_reply": "2021-06-14T01:05:24.564246Z",
     "shell.execute_reply.started": "2021-06-14T01:00:15.648867Z"
    },
    "papermill": {
     "duration": 0.038571,
     "end_time": "2021-06-14T01:05:24.564940",
     "exception": false,
     "start_time": "2021-06-14T01:05:24.526369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Every Xth epoch, generate and save 3 images. \n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=6, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "          random_latent_vectors = np.random.rand(self.num_img, 100).astype(np.float32)\n",
    "          generated_images = self.model.generator(random_latent_vectors)\n",
    "\n",
    "          generated_images = (generated_images + 1) / 2.0\n",
    "\n",
    "          for i in range(self.num_img):\n",
    "            plt.imshow(generated_images[i])\n",
    "            plt.axis('off')\n",
    "            name = str(epoch) + '_generated_image_' + str(i) + '.png'\n",
    "            plt.savefig(name, bbox_inches='tight')\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "israeli-things",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T01:05:24.695388Z",
     "iopub.status.busy": "2021-06-14T01:05:24.694672Z",
     "iopub.status.idle": "2021-06-14T07:47:48.162235Z",
     "shell.execute_reply": "2021-06-14T07:47:48.162830Z",
     "shell.execute_reply.started": "2021-06-14T01:00:18.788311Z"
    },
    "papermill": {
     "duration": 24143.509908,
     "end_time": "2021-06-14T07:47:48.163094",
     "exception": false,
     "start_time": "2021-06-14T01:05:24.653186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering scope\n",
      "creating model\n",
      "compiling\n",
      "starting training\n",
      "Epoch 1/800\n",
      "42/42 [==============================] - 116s 1s/step - d_loss: -1.1012 - g_loss: 0.5927\n",
      "Epoch 2/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.9869 - g_loss: -0.7568\n",
      "Epoch 3/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.8853 - g_loss: -1.3166\n",
      "Epoch 4/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.0613 - g_loss: 1.6281\n",
      "Epoch 5/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.0303 - g_loss: 4.6299\n",
      "Epoch 6/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.0539 - g_loss: 2.5765\n",
      "Epoch 7/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.8581 - g_loss: 4.3635\n",
      "Epoch 8/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.0931 - g_loss: 2.3707\n",
      "Epoch 9/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.6714 - g_loss: -0.6955\n",
      "Epoch 10/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.8393 - g_loss: 0.8207\n",
      "Epoch 11/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.0623 - g_loss: 2.7544\n",
      "Epoch 12/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.8926 - g_loss: 1.3291\n",
      "Epoch 13/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.8977 - g_loss: 0.5944\n",
      "Epoch 14/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.9294 - g_loss: 0.9008\n",
      "Epoch 15/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.9406 - g_loss: -0.5895\n",
      "Epoch 16/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.8582 - g_loss: 5.5016\n",
      "Epoch 17/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.9272 - g_loss: -0.2626\n",
      "Epoch 18/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.0188 - g_loss: -3.0215\n",
      "Epoch 19/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.7371 - g_loss: 0.0437\n",
      "Epoch 20/800\n",
      "42/42 [==============================] - 31s 734ms/step - d_loss: -0.7480 - g_loss: 0.7117\n",
      "Epoch 21/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.8322 - g_loss: 1.6768\n",
      "Epoch 22/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.8272 - g_loss: -0.6869\n",
      "Epoch 23/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.9150 - g_loss: 0.4664\n",
      "Epoch 24/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.7546 - g_loss: -1.9209\n",
      "Epoch 25/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.9697 - g_loss: -0.5725\n",
      "Epoch 26/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.8956 - g_loss: -1.4594\n",
      "Epoch 27/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.6748 - g_loss: 0.5368\n",
      "Epoch 28/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.5883 - g_loss: -1.3566\n",
      "Epoch 29/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.7718 - g_loss: -0.7960\n",
      "Epoch 30/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.9372 - g_loss: 0.2841\n",
      "Epoch 31/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.5536 - g_loss: -0.6170\n",
      "Epoch 32/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.6365 - g_loss: -2.5245\n",
      "Epoch 33/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.0814 - g_loss: -1.4571\n",
      "Epoch 34/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.5645 - g_loss: 9.6566\n",
      "Epoch 35/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.7284 - g_loss: -2.9894\n",
      "Epoch 36/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.7441 - g_loss: 1.8140\n",
      "Epoch 37/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.8378 - g_loss: 0.6774\n",
      "Epoch 38/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.8117 - g_loss: -2.6948\n",
      "Epoch 39/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.6381 - g_loss: 2.5797\n",
      "Epoch 40/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -0.8079 - g_loss: -0.6555\n",
      "Epoch 41/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.6602 - g_loss: -6.6245\n",
      "Epoch 42/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.0039 - g_loss: 0.8910\n",
      "Epoch 43/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.7590 - g_loss: 1.1243\n",
      "Epoch 44/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.8183 - g_loss: 2.2035\n",
      "Epoch 45/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.7018 - g_loss: 0.9473\n",
      "Epoch 46/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -0.8455 - g_loss: 0.9869\n",
      "Epoch 47/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -0.8738 - g_loss: -1.7272\n",
      "Epoch 48/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.6961 - g_loss: -2.5153\n",
      "Epoch 49/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.8756 - g_loss: 1.2945\n",
      "Epoch 50/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.6175 - g_loss: 0.9710\n",
      "Epoch 51/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.7585 - g_loss: 2.0240\n",
      "Epoch 52/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.8867 - g_loss: 1.6085\n",
      "Epoch 53/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.0628 - g_loss: 1.4634\n",
      "Epoch 54/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.6724 - g_loss: 1.4305\n",
      "Epoch 55/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.7785 - g_loss: 1.0251\n",
      "Epoch 56/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -0.7546 - g_loss: 2.0087\n",
      "Epoch 57/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -0.6846 - g_loss: 1.7891\n",
      "Epoch 58/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.0299 - g_loss: -0.1699\n",
      "Epoch 59/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.1807 - g_loss: 0.5079\n",
      "Epoch 60/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -1.0735 - g_loss: -0.0325\n",
      "Epoch 61/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.4519 - g_loss: 0.1210\n",
      "Epoch 62/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.2385 - g_loss: -2.0861\n",
      "Epoch 63/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.2773 - g_loss: -1.8563\n",
      "Epoch 64/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.2397 - g_loss: -2.2783\n",
      "Epoch 65/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.4524 - g_loss: -4.2132\n",
      "Epoch 66/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.5423 - g_loss: -4.8127\n",
      "Epoch 67/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.8157 - g_loss: -5.9857\n",
      "Epoch 68/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.7895 - g_loss: -6.5871\n",
      "Epoch 69/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.0977 - g_loss: -6.8048\n",
      "Epoch 70/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.9843 - g_loss: -6.4214\n",
      "Epoch 71/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.8562 - g_loss: -8.1697\n",
      "Epoch 72/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.9188 - g_loss: -8.2610\n",
      "Epoch 73/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1257 - g_loss: -8.8330\n",
      "Epoch 74/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.0074 - g_loss: -7.8184\n",
      "Epoch 75/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -1.9994 - g_loss: -8.3980\n",
      "Epoch 76/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1458 - g_loss: -8.8924\n",
      "Epoch 77/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.0927 - g_loss: -8.3042\n",
      "Epoch 78/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.0690 - g_loss: -8.7319\n",
      "Epoch 79/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1480 - g_loss: -8.5986\n",
      "Epoch 80/800\n",
      "42/42 [==============================] - 31s 730ms/step - d_loss: -2.0842 - g_loss: -7.6401\n",
      "Epoch 81/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.0647 - g_loss: -8.0050\n",
      "Epoch 82/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.0371 - g_loss: -8.4148\n",
      "Epoch 83/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1737 - g_loss: -8.0313\n",
      "Epoch 84/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1629 - g_loss: -7.5922\n",
      "Epoch 85/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1451 - g_loss: -7.4472\n",
      "Epoch 86/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1389 - g_loss: -7.6638\n",
      "Epoch 87/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1806 - g_loss: -7.6969\n",
      "Epoch 88/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1328 - g_loss: -7.0919\n",
      "Epoch 89/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1707 - g_loss: -6.6157\n",
      "Epoch 90/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2317 - g_loss: -6.8433\n",
      "Epoch 91/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1246 - g_loss: -7.0821\n",
      "Epoch 92/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2497 - g_loss: -6.9634\n",
      "Epoch 93/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2894 - g_loss: -6.8662\n",
      "Epoch 94/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.0916 - g_loss: -6.5625\n",
      "Epoch 95/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1303 - g_loss: -6.6011\n",
      "Epoch 96/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1214 - g_loss: -6.8414\n",
      "Epoch 97/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1813 - g_loss: -6.5266\n",
      "Epoch 98/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1721 - g_loss: -6.3196\n",
      "Epoch 99/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1209 - g_loss: -6.2492\n",
      "Epoch 100/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -2.1795 - g_loss: -6.1559\n",
      "Epoch 101/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1223 - g_loss: -5.7611\n",
      "Epoch 102/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2225 - g_loss: -5.9197\n",
      "Epoch 103/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1622 - g_loss: -5.6263\n",
      "Epoch 104/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2150 - g_loss: -5.6694\n",
      "Epoch 105/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1424 - g_loss: -5.4294\n",
      "Epoch 106/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2083 - g_loss: -5.4752\n",
      "Epoch 107/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1182 - g_loss: -5.4891\n",
      "Epoch 108/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1827 - g_loss: -5.5056\n",
      "Epoch 109/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1304 - g_loss: -5.5092\n",
      "Epoch 110/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1645 - g_loss: -5.6115\n",
      "Epoch 111/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2240 - g_loss: -5.1824\n",
      "Epoch 112/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1417 - g_loss: -5.5525\n",
      "Epoch 113/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2155 - g_loss: -5.2282\n",
      "Epoch 114/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1744 - g_loss: -5.0293\n",
      "Epoch 115/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2591 - g_loss: -5.1204\n",
      "Epoch 116/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1470 - g_loss: -4.7445\n",
      "Epoch 117/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.1882 - g_loss: -5.2757\n",
      "Epoch 118/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1461 - g_loss: -4.9137\n",
      "Epoch 119/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2066 - g_loss: -4.4883\n",
      "Epoch 120/800\n",
      "42/42 [==============================] - 31s 730ms/step - d_loss: -2.1410 - g_loss: -4.6936\n",
      "Epoch 121/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1135 - g_loss: -4.7836\n",
      "Epoch 122/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2106 - g_loss: -4.5086\n",
      "Epoch 123/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2512 - g_loss: -4.3056\n",
      "Epoch 124/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1541 - g_loss: -4.4978\n",
      "Epoch 125/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3054 - g_loss: -4.2013\n",
      "Epoch 126/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.0616 - g_loss: -4.4854\n",
      "Epoch 127/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.1548 - g_loss: -4.3502\n",
      "Epoch 128/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2778 - g_loss: -4.2714\n",
      "Epoch 129/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2057 - g_loss: -4.0748\n",
      "Epoch 130/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1602 - g_loss: -4.1383\n",
      "Epoch 131/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1840 - g_loss: -4.3345\n",
      "Epoch 132/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1668 - g_loss: -4.2944\n",
      "Epoch 133/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2378 - g_loss: -4.1875\n",
      "Epoch 134/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1810 - g_loss: -4.0746\n",
      "Epoch 135/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1827 - g_loss: -3.9152\n",
      "Epoch 136/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2289 - g_loss: -4.0167\n",
      "Epoch 137/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1299 - g_loss: -3.8809\n",
      "Epoch 138/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1949 - g_loss: -3.5474\n",
      "Epoch 139/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1457 - g_loss: -3.9025\n",
      "Epoch 140/800\n",
      "42/42 [==============================] - 31s 730ms/step - d_loss: -2.1989 - g_loss: -3.8790\n",
      "Epoch 141/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1174 - g_loss: -3.9141\n",
      "Epoch 142/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1883 - g_loss: -3.5731\n",
      "Epoch 143/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2180 - g_loss: -3.7857\n",
      "Epoch 144/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.0674 - g_loss: -3.8542\n",
      "Epoch 145/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2192 - g_loss: -3.4525\n",
      "Epoch 146/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1296 - g_loss: -3.2324\n",
      "Epoch 147/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.1751 - g_loss: -3.5097\n",
      "Epoch 148/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2564 - g_loss: -3.5765\n",
      "Epoch 149/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1156 - g_loss: -3.5923\n",
      "Epoch 150/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1805 - g_loss: -3.1519\n",
      "Epoch 151/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1257 - g_loss: -3.4223\n",
      "Epoch 152/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1095 - g_loss: -2.9351\n",
      "Epoch 153/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1713 - g_loss: -2.9324\n",
      "Epoch 154/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1727 - g_loss: -3.0075\n",
      "Epoch 155/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2205 - g_loss: -3.1497\n",
      "Epoch 156/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1698 - g_loss: -3.1292\n",
      "Epoch 157/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1936 - g_loss: -3.1839\n",
      "Epoch 158/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2865 - g_loss: -3.2444\n",
      "Epoch 159/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1868 - g_loss: -2.9668\n",
      "Epoch 160/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -2.2134 - g_loss: -3.0865\n",
      "Epoch 161/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1469 - g_loss: -2.8404\n",
      "Epoch 162/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2326 - g_loss: -2.7034\n",
      "Epoch 163/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2083 - g_loss: -2.9116\n",
      "Epoch 164/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2085 - g_loss: -2.9211\n",
      "Epoch 165/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2201 - g_loss: -2.6897\n",
      "Epoch 166/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2145 - g_loss: -2.8661\n",
      "Epoch 167/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2166 - g_loss: -2.8009\n",
      "Epoch 168/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1197 - g_loss: -2.6492\n",
      "Epoch 169/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2377 - g_loss: -2.6735\n",
      "Epoch 170/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1937 - g_loss: -2.3622\n",
      "Epoch 171/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1910 - g_loss: -2.6641\n",
      "Epoch 172/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1187 - g_loss: -2.6865\n",
      "Epoch 173/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1611 - g_loss: -2.5718\n",
      "Epoch 174/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1907 - g_loss: -2.5869\n",
      "Epoch 175/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2673 - g_loss: -2.5843\n",
      "Epoch 176/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2321 - g_loss: -2.5004\n",
      "Epoch 177/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1963 - g_loss: -2.4958\n",
      "Epoch 178/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2007 - g_loss: -2.3525\n",
      "Epoch 179/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2660 - g_loss: -2.4945\n",
      "Epoch 180/800\n",
      "42/42 [==============================] - 31s 730ms/step - d_loss: -2.1885 - g_loss: -2.5887\n",
      "Epoch 181/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2243 - g_loss: -2.3954\n",
      "Epoch 182/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1523 - g_loss: -2.4376\n",
      "Epoch 183/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1684 - g_loss: -2.5018\n",
      "Epoch 184/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1582 - g_loss: -2.1568\n",
      "Epoch 185/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1792 - g_loss: -2.4509\n",
      "Epoch 186/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1770 - g_loss: -2.3241\n",
      "Epoch 187/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1229 - g_loss: -2.3491\n",
      "Epoch 188/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2582 - g_loss: -2.2492\n",
      "Epoch 189/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2685 - g_loss: -2.2251\n",
      "Epoch 190/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2095 - g_loss: -2.1354\n",
      "Epoch 191/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2394 - g_loss: -2.1915\n",
      "Epoch 192/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1829 - g_loss: -2.1839\n",
      "Epoch 193/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2826 - g_loss: -2.1411\n",
      "Epoch 194/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2547 - g_loss: -2.0618\n",
      "Epoch 195/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2685 - g_loss: -2.0530\n",
      "Epoch 196/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1591 - g_loss: -1.9454\n",
      "Epoch 197/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1654 - g_loss: -1.9241\n",
      "Epoch 198/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2197 - g_loss: -2.2195\n",
      "Epoch 199/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2541 - g_loss: -2.1109\n",
      "Epoch 200/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -2.2174 - g_loss: -1.8366\n",
      "Epoch 201/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2027 - g_loss: -2.0183\n",
      "Epoch 202/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1463 - g_loss: -2.1336\n",
      "Epoch 203/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2117 - g_loss: -2.1681\n",
      "Epoch 204/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2460 - g_loss: -1.9688\n",
      "Epoch 205/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2188 - g_loss: -2.1956\n",
      "Epoch 206/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3088 - g_loss: -1.7696\n",
      "Epoch 207/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2620 - g_loss: -1.9806\n",
      "Epoch 208/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1432 - g_loss: -1.9297\n",
      "Epoch 209/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2552 - g_loss: -1.9315\n",
      "Epoch 210/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3093 - g_loss: -1.7701\n",
      "Epoch 211/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2920 - g_loss: -1.8519\n",
      "Epoch 212/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1914 - g_loss: -1.7238\n",
      "Epoch 213/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2068 - g_loss: -1.6755\n",
      "Epoch 214/800\n",
      "42/42 [==============================] - 30s 716ms/step - d_loss: -2.1960 - g_loss: -1.8046\n",
      "Epoch 215/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3197 - g_loss: -1.6441\n",
      "Epoch 216/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2380 - g_loss: -1.9084\n",
      "Epoch 217/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2888 - g_loss: -1.6527\n",
      "Epoch 218/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2673 - g_loss: -1.7823\n",
      "Epoch 219/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2845 - g_loss: -1.6887\n",
      "Epoch 220/800\n",
      "42/42 [==============================] - 31s 730ms/step - d_loss: -2.2588 - g_loss: -1.5750\n",
      "Epoch 221/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.3451 - g_loss: -1.5700\n",
      "Epoch 222/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2858 - g_loss: -1.4201\n",
      "Epoch 223/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.1920 - g_loss: -1.6915\n",
      "Epoch 224/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2698 - g_loss: -1.6041\n",
      "Epoch 225/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2001 - g_loss: -1.4784\n",
      "Epoch 226/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3041 - g_loss: -1.3672\n",
      "Epoch 227/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2729 - g_loss: -1.7989\n",
      "Epoch 228/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2482 - g_loss: -1.6584\n",
      "Epoch 229/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2448 - g_loss: -1.6078\n",
      "Epoch 230/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3010 - g_loss: -1.4817\n",
      "Epoch 231/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2595 - g_loss: -1.5491\n",
      "Epoch 232/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3033 - g_loss: -1.5301\n",
      "Epoch 233/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3210 - g_loss: -1.5100\n",
      "Epoch 234/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3032 - g_loss: -1.5075\n",
      "Epoch 235/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3043 - g_loss: -1.4108\n",
      "Epoch 236/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2919 - g_loss: -1.4416\n",
      "Epoch 237/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2913 - g_loss: -1.5752\n",
      "Epoch 238/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2640 - g_loss: -1.4389\n",
      "Epoch 239/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2679 - g_loss: -1.4087\n",
      "Epoch 240/800\n",
      "42/42 [==============================] - 31s 730ms/step - d_loss: -2.3089 - g_loss: -1.3063\n",
      "Epoch 241/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2805 - g_loss: -1.3043\n",
      "Epoch 242/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2275 - g_loss: -1.3553\n",
      "Epoch 243/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3272 - g_loss: -1.3654\n",
      "Epoch 244/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2953 - g_loss: -1.1987\n",
      "Epoch 245/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1937 - g_loss: -1.2763\n",
      "Epoch 246/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1805 - g_loss: -1.3551\n",
      "Epoch 247/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2594 - g_loss: -1.3204\n",
      "Epoch 248/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3288 - g_loss: -1.2814\n",
      "Epoch 249/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3228 - g_loss: -1.1978\n",
      "Epoch 250/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2909 - g_loss: -1.2526\n",
      "Epoch 251/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3170 - g_loss: -1.3030\n",
      "Epoch 252/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3341 - g_loss: -1.2604\n",
      "Epoch 253/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3121 - g_loss: -1.1518\n",
      "Epoch 254/800\n",
      "42/42 [==============================] - 30s 716ms/step - d_loss: -2.2200 - g_loss: -1.0224\n",
      "Epoch 255/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2999 - g_loss: -1.1709\n",
      "Epoch 256/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3319 - g_loss: -1.1681\n",
      "Epoch 257/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2745 - g_loss: -1.0966\n",
      "Epoch 258/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3093 - g_loss: -1.0410\n",
      "Epoch 259/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1972 - g_loss: -1.2339\n",
      "Epoch 260/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -2.2288 - g_loss: -1.0744\n",
      "Epoch 261/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2658 - g_loss: -1.1839\n",
      "Epoch 262/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2815 - g_loss: -1.0712\n",
      "Epoch 263/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3058 - g_loss: -1.1038\n",
      "Epoch 264/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.4045 - g_loss: -1.1010\n",
      "Epoch 265/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1986 - g_loss: -1.0620\n",
      "Epoch 266/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2487 - g_loss: -0.9601\n",
      "Epoch 267/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2237 - g_loss: -1.0583\n",
      "Epoch 268/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1665 - g_loss: -1.0628\n",
      "Epoch 269/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2286 - g_loss: -1.1098\n",
      "Epoch 270/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2621 - g_loss: -1.0986\n",
      "Epoch 271/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2405 - g_loss: -1.0856\n",
      "Epoch 272/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2645 - g_loss: -1.0232\n",
      "Epoch 273/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2999 - g_loss: -1.0860\n",
      "Epoch 274/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3909 - g_loss: -1.0501\n",
      "Epoch 275/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2424 - g_loss: -0.9610\n",
      "Epoch 276/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3100 - g_loss: -0.9797\n",
      "Epoch 277/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2605 - g_loss: -0.9735\n",
      "Epoch 278/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3216 - g_loss: -1.1599\n",
      "Epoch 279/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2513 - g_loss: -1.1472\n",
      "Epoch 280/800\n",
      "42/42 [==============================] - 31s 730ms/step - d_loss: -2.1550 - g_loss: -1.1005\n",
      "Epoch 281/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2648 - g_loss: -0.9544\n",
      "Epoch 282/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3046 - g_loss: -0.9324\n",
      "Epoch 283/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2731 - g_loss: -1.0441\n",
      "Epoch 284/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3166 - g_loss: -1.1557\n",
      "Epoch 285/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2270 - g_loss: -1.0420\n",
      "Epoch 286/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1842 - g_loss: -1.1642\n",
      "Epoch 287/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1892 - g_loss: -1.0732\n",
      "Epoch 288/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2765 - g_loss: -1.0497\n",
      "Epoch 289/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2892 - g_loss: -1.2175\n",
      "Epoch 290/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3479 - g_loss: -1.1638\n",
      "Epoch 291/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2835 - g_loss: -1.1047\n",
      "Epoch 292/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2277 - g_loss: -1.0129\n",
      "Epoch 293/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3210 - g_loss: -1.1523\n",
      "Epoch 294/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2753 - g_loss: -1.0671\n",
      "Epoch 295/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2206 - g_loss: -1.1218\n",
      "Epoch 296/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2919 - g_loss: -1.1443\n",
      "Epoch 297/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2526 - g_loss: -1.0814\n",
      "Epoch 298/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2970 - g_loss: -1.2173\n",
      "Epoch 299/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2963 - g_loss: -1.2086\n",
      "Epoch 300/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -2.3499 - g_loss: -1.0157\n",
      "Epoch 301/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3323 - g_loss: -1.0830\n",
      "Epoch 302/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2904 - g_loss: -1.1521\n",
      "Epoch 303/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2381 - g_loss: -1.2203\n",
      "Epoch 304/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2823 - g_loss: -0.9437\n",
      "Epoch 305/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2040 - g_loss: -1.0926\n",
      "Epoch 306/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3413 - g_loss: -1.0876\n",
      "Epoch 307/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3086 - g_loss: -1.0503\n",
      "Epoch 308/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2613 - g_loss: -1.1399\n",
      "Epoch 309/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2097 - g_loss: -1.1340\n",
      "Epoch 310/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2473 - g_loss: -1.1433\n",
      "Epoch 311/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1961 - g_loss: -1.0619\n",
      "Epoch 312/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2348 - g_loss: -1.1928\n",
      "Epoch 313/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2649 - g_loss: -0.9869\n",
      "Epoch 314/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3259 - g_loss: -1.0119\n",
      "Epoch 315/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2918 - g_loss: -0.9889\n",
      "Epoch 316/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2831 - g_loss: -1.0809\n",
      "Epoch 317/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2669 - g_loss: -1.0445\n",
      "Epoch 318/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3239 - g_loss: -0.9269\n",
      "Epoch 319/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2672 - g_loss: -1.0798\n",
      "Epoch 320/800\n",
      "42/42 [==============================] - 31s 732ms/step - d_loss: -2.3161 - g_loss: -0.9972\n",
      "Epoch 321/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2697 - g_loss: -0.9997\n",
      "Epoch 322/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2368 - g_loss: -0.9012\n",
      "Epoch 323/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3013 - g_loss: -0.9460\n",
      "Epoch 324/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3021 - g_loss: -0.8890\n",
      "Epoch 325/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2821 - g_loss: -1.1394\n",
      "Epoch 326/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2792 - g_loss: -1.1040\n",
      "Epoch 327/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2346 - g_loss: -1.1954\n",
      "Epoch 328/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1628 - g_loss: -0.9294\n",
      "Epoch 329/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1860 - g_loss: -1.0282\n",
      "Epoch 330/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2325 - g_loss: -0.9578\n",
      "Epoch 331/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2488 - g_loss: -0.8615\n",
      "Epoch 332/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2137 - g_loss: -0.9952\n",
      "Epoch 333/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2127 - g_loss: -1.0080\n",
      "Epoch 334/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2941 - g_loss: -0.9180\n",
      "Epoch 335/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3099 - g_loss: -0.9082\n",
      "Epoch 336/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1912 - g_loss: -0.8787\n",
      "Epoch 337/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2449 - g_loss: -0.8735\n",
      "Epoch 338/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2797 - g_loss: -0.9597\n",
      "Epoch 339/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2355 - g_loss: -0.8370\n",
      "Epoch 340/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -2.2821 - g_loss: -0.9013\n",
      "Epoch 341/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2029 - g_loss: -1.0751\n",
      "Epoch 342/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2504 - g_loss: -0.8325\n",
      "Epoch 343/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2942 - g_loss: -0.8581\n",
      "Epoch 344/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2227 - g_loss: -0.8186\n",
      "Epoch 345/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2228 - g_loss: -0.9476\n",
      "Epoch 346/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3315 - g_loss: -0.9048\n",
      "Epoch 347/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2557 - g_loss: -0.8657\n",
      "Epoch 348/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1314 - g_loss: -0.9087\n",
      "Epoch 349/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1985 - g_loss: -0.8960\n",
      "Epoch 350/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1898 - g_loss: -0.8909\n",
      "Epoch 351/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1619 - g_loss: -1.0760\n",
      "Epoch 352/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2688 - g_loss: -0.9511\n",
      "Epoch 353/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2376 - g_loss: -0.8994\n",
      "Epoch 354/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2165 - g_loss: -0.7802\n",
      "Epoch 355/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2992 - g_loss: -0.9263\n",
      "Epoch 356/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2700 - g_loss: -0.9011\n",
      "Epoch 357/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2220 - g_loss: -1.0661\n",
      "Epoch 358/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2753 - g_loss: -0.8950\n",
      "Epoch 359/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2476 - g_loss: -0.8673\n",
      "Epoch 360/800\n",
      "42/42 [==============================] - 31s 730ms/step - d_loss: -2.2995 - g_loss: -0.8482\n",
      "Epoch 361/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2020 - g_loss: -0.9016\n",
      "Epoch 362/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2955 - g_loss: -0.9638\n",
      "Epoch 363/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2694 - g_loss: -0.8082\n",
      "Epoch 364/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2098 - g_loss: -0.9063\n",
      "Epoch 365/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3011 - g_loss: -1.0376\n",
      "Epoch 366/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2781 - g_loss: -1.0010\n",
      "Epoch 367/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2283 - g_loss: -0.9267\n",
      "Epoch 368/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2309 - g_loss: -0.9927\n",
      "Epoch 369/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2554 - g_loss: -0.8753\n",
      "Epoch 370/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3544 - g_loss: -0.9056\n",
      "Epoch 371/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2776 - g_loss: -0.9742\n",
      "Epoch 372/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2755 - g_loss: -0.8628\n",
      "Epoch 373/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2400 - g_loss: -0.8848\n",
      "Epoch 374/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2486 - g_loss: -0.9051\n",
      "Epoch 375/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2463 - g_loss: -0.9893\n",
      "Epoch 376/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2432 - g_loss: -0.8755\n",
      "Epoch 377/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3103 - g_loss: -0.9194\n",
      "Epoch 378/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2561 - g_loss: -0.9606\n",
      "Epoch 379/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3213 - g_loss: -0.9958\n",
      "Epoch 380/800\n",
      "42/42 [==============================] - 31s 731ms/step - d_loss: -2.2870 - g_loss: -0.9757\n",
      "Epoch 381/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2689 - g_loss: -1.0465\n",
      "Epoch 382/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1791 - g_loss: -0.8949\n",
      "Epoch 383/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2769 - g_loss: -0.9481\n",
      "Epoch 384/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3031 - g_loss: -0.9759\n",
      "Epoch 385/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2759 - g_loss: -0.9604\n",
      "Epoch 386/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3244 - g_loss: -0.9240\n",
      "Epoch 387/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2235 - g_loss: -0.7352\n",
      "Epoch 388/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2299 - g_loss: -0.8029\n",
      "Epoch 389/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2244 - g_loss: -0.9701\n",
      "Epoch 390/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2787 - g_loss: -1.0500\n",
      "Epoch 391/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2847 - g_loss: -1.0427\n",
      "Epoch 392/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2846 - g_loss: -0.9671\n",
      "Epoch 393/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2826 - g_loss: -0.8606\n",
      "Epoch 394/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2749 - g_loss: -0.9218\n",
      "Epoch 395/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3743 - g_loss: -0.8913\n",
      "Epoch 396/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2265 - g_loss: -0.8158\n",
      "Epoch 397/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2701 - g_loss: -0.8677\n",
      "Epoch 398/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.4045 - g_loss: -0.8924\n",
      "Epoch 399/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3439 - g_loss: -0.8667\n",
      "Epoch 400/800\n",
      "42/42 [==============================] - 31s 730ms/step - d_loss: -2.2504 - g_loss: -0.9712\n",
      "Epoch 401/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3118 - g_loss: -1.0081\n",
      "Epoch 402/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2280 - g_loss: -0.9572\n",
      "Epoch 403/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2190 - g_loss: -1.0400\n",
      "Epoch 404/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1820 - g_loss: -0.9871\n",
      "Epoch 405/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2874 - g_loss: -0.9895\n",
      "Epoch 406/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3018 - g_loss: -0.9412\n",
      "Epoch 407/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2308 - g_loss: -1.0692\n",
      "Epoch 408/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2177 - g_loss: -0.9953\n",
      "Epoch 409/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2072 - g_loss: -0.8722\n",
      "Epoch 410/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2108 - g_loss: -0.8718\n",
      "Epoch 411/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2739 - g_loss: -1.0056\n",
      "Epoch 412/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2680 - g_loss: -0.8285\n",
      "Epoch 413/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3001 - g_loss: -0.8672\n",
      "Epoch 414/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3363 - g_loss: -0.9106\n",
      "Epoch 415/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2552 - g_loss: -0.8830\n",
      "Epoch 416/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2032 - g_loss: -0.9766\n",
      "Epoch 417/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1739 - g_loss: -0.9548\n",
      "Epoch 418/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1995 - g_loss: -0.7783\n",
      "Epoch 419/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2142 - g_loss: -0.7295\n",
      "Epoch 420/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -2.2493 - g_loss: -0.8588\n",
      "Epoch 421/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2169 - g_loss: -0.9876\n",
      "Epoch 422/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1520 - g_loss: -1.0268\n",
      "Epoch 423/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2914 - g_loss: -0.9551\n",
      "Epoch 424/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2282 - g_loss: -0.8961\n",
      "Epoch 425/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1954 - g_loss: -0.9232\n",
      "Epoch 426/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1918 - g_loss: -0.8126\n",
      "Epoch 427/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2537 - g_loss: -0.8368\n",
      "Epoch 428/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3026 - g_loss: -0.8684\n",
      "Epoch 429/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1997 - g_loss: -1.0213\n",
      "Epoch 430/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3106 - g_loss: -1.0436\n",
      "Epoch 431/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2683 - g_loss: -0.9686\n",
      "Epoch 432/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2340 - g_loss: -0.9992\n",
      "Epoch 433/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1760 - g_loss: -1.0482\n",
      "Epoch 434/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2730 - g_loss: -0.9559\n",
      "Epoch 435/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1968 - g_loss: -1.0747\n",
      "Epoch 436/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1471 - g_loss: -0.9835\n",
      "Epoch 437/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2138 - g_loss: -1.0153\n",
      "Epoch 438/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2740 - g_loss: -1.0186\n",
      "Epoch 439/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2133 - g_loss: -1.0317\n",
      "Epoch 440/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -2.2496 - g_loss: -0.8160\n",
      "Epoch 441/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2143 - g_loss: -0.8145\n",
      "Epoch 442/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2427 - g_loss: -0.8293\n",
      "Epoch 443/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2859 - g_loss: -0.8270\n",
      "Epoch 444/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3004 - g_loss: -0.8434\n",
      "Epoch 445/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3000 - g_loss: -0.8325\n",
      "Epoch 446/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2245 - g_loss: -0.8713\n",
      "Epoch 447/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2658 - g_loss: -0.7993\n",
      "Epoch 448/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2819 - g_loss: -0.8797\n",
      "Epoch 449/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2312 - g_loss: -0.7132\n",
      "Epoch 450/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2741 - g_loss: -0.6892\n",
      "Epoch 451/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2298 - g_loss: -0.8277\n",
      "Epoch 452/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2253 - g_loss: -0.7374\n",
      "Epoch 453/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2923 - g_loss: -0.7456\n",
      "Epoch 454/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3118 - g_loss: -0.8441\n",
      "Epoch 455/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1814 - g_loss: -0.9742\n",
      "Epoch 456/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.3160 - g_loss: -0.8524\n",
      "Epoch 457/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2965 - g_loss: -0.8924\n",
      "Epoch 458/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1625 - g_loss: -0.9006\n",
      "Epoch 459/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2556 - g_loss: -1.0341\n",
      "Epoch 460/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -2.1847 - g_loss: -0.8551\n",
      "Epoch 461/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2323 - g_loss: -0.8238\n",
      "Epoch 462/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1704 - g_loss: -0.7783\n",
      "Epoch 463/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.0876 - g_loss: -0.8597\n",
      "Epoch 464/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2838 - g_loss: -1.0424\n",
      "Epoch 465/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1939 - g_loss: -0.8947\n",
      "Epoch 466/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2160 - g_loss: -0.9213\n",
      "Epoch 467/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2442 - g_loss: -0.7805\n",
      "Epoch 468/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2713 - g_loss: -0.8313\n",
      "Epoch 469/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1838 - g_loss: -0.9815\n",
      "Epoch 470/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2385 - g_loss: -0.8614\n",
      "Epoch 471/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3259 - g_loss: -0.8454\n",
      "Epoch 472/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3004 - g_loss: -0.9597\n",
      "Epoch 473/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2318 - g_loss: -0.8775\n",
      "Epoch 474/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2459 - g_loss: -1.0423\n",
      "Epoch 475/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3419 - g_loss: -0.9120\n",
      "Epoch 476/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2380 - g_loss: -0.8660\n",
      "Epoch 477/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2355 - g_loss: -0.9487\n",
      "Epoch 478/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2236 - g_loss: -0.8538\n",
      "Epoch 479/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1888 - g_loss: -0.7738\n",
      "Epoch 480/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -2.3170 - g_loss: -0.8406\n",
      "Epoch 481/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1888 - g_loss: -0.8466\n",
      "Epoch 482/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2384 - g_loss: -0.9621\n",
      "Epoch 483/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2497 - g_loss: -0.8599\n",
      "Epoch 484/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2333 - g_loss: -0.8961\n",
      "Epoch 485/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1315 - g_loss: -0.9825\n",
      "Epoch 486/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2276 - g_loss: -0.9817\n",
      "Epoch 487/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1846 - g_loss: -0.8594\n",
      "Epoch 488/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2584 - g_loss: -1.1044\n",
      "Epoch 489/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2948 - g_loss: -1.0257\n",
      "Epoch 490/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2381 - g_loss: -0.9805\n",
      "Epoch 491/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2530 - g_loss: -0.8765\n",
      "Epoch 492/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2478 - g_loss: -1.0105\n",
      "Epoch 493/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2583 - g_loss: -1.1075\n",
      "Epoch 494/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2604 - g_loss: -1.0282\n",
      "Epoch 495/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2028 - g_loss: -0.8315\n",
      "Epoch 496/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2319 - g_loss: -0.8986\n",
      "Epoch 497/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1976 - g_loss: -0.9153\n",
      "Epoch 498/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2489 - g_loss: -0.9422\n",
      "Epoch 499/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2608 - g_loss: -1.0043\n",
      "Epoch 500/800\n",
      "42/42 [==============================] - 31s 730ms/step - d_loss: -2.1714 - g_loss: -0.9569\n",
      "Epoch 501/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2132 - g_loss: -0.9858\n",
      "Epoch 502/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2495 - g_loss: -0.8666\n",
      "Epoch 503/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2492 - g_loss: -0.9899\n",
      "Epoch 504/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2413 - g_loss: -0.9432\n",
      "Epoch 505/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1677 - g_loss: -0.9735\n",
      "Epoch 506/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3117 - g_loss: -0.9334\n",
      "Epoch 507/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1914 - g_loss: -1.0265\n",
      "Epoch 508/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2356 - g_loss: -0.8699\n",
      "Epoch 509/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2712 - g_loss: -0.9969\n",
      "Epoch 510/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2237 - g_loss: -0.9183\n",
      "Epoch 511/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1704 - g_loss: -1.0000\n",
      "Epoch 512/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.1748 - g_loss: -0.8926\n",
      "Epoch 513/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1508 - g_loss: -0.9140\n",
      "Epoch 514/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2742 - g_loss: -0.7428\n",
      "Epoch 515/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1559 - g_loss: -0.8824\n",
      "Epoch 516/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.1860 - g_loss: -0.7258\n",
      "Epoch 517/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1688 - g_loss: -0.9493\n",
      "Epoch 518/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2657 - g_loss: -0.9167\n",
      "Epoch 519/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2164 - g_loss: -0.9429\n",
      "Epoch 520/800\n",
      "42/42 [==============================] - 31s 730ms/step - d_loss: -2.1848 - g_loss: -1.0129\n",
      "Epoch 521/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1835 - g_loss: -0.8621\n",
      "Epoch 522/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1282 - g_loss: -0.8431\n",
      "Epoch 523/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2252 - g_loss: -0.9905\n",
      "Epoch 524/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2304 - g_loss: -0.9062\n",
      "Epoch 525/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2972 - g_loss: -0.9960\n",
      "Epoch 526/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2581 - g_loss: -1.0358\n",
      "Epoch 527/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2891 - g_loss: -0.9063\n",
      "Epoch 528/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1888 - g_loss: -0.8869\n",
      "Epoch 529/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1980 - g_loss: -0.8894\n",
      "Epoch 530/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1736 - g_loss: -0.9772\n",
      "Epoch 531/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2442 - g_loss: -0.7529\n",
      "Epoch 532/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2061 - g_loss: -1.0263\n",
      "Epoch 533/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1993 - g_loss: -0.8856\n",
      "Epoch 534/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2542 - g_loss: -1.0088\n",
      "Epoch 535/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2481 - g_loss: -0.9165\n",
      "Epoch 536/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2095 - g_loss: -0.9692\n",
      "Epoch 537/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1898 - g_loss: -0.9492\n",
      "Epoch 538/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2692 - g_loss: -0.8625\n",
      "Epoch 539/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1588 - g_loss: -1.1225\n",
      "Epoch 540/800\n",
      "42/42 [==============================] - 31s 731ms/step - d_loss: -2.2477 - g_loss: -0.8542\n",
      "Epoch 541/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1472 - g_loss: -0.9494\n",
      "Epoch 542/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2432 - g_loss: -0.9796\n",
      "Epoch 543/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2153 - g_loss: -1.1076\n",
      "Epoch 544/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1759 - g_loss: -0.8711\n",
      "Epoch 545/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2087 - g_loss: -0.8941\n",
      "Epoch 546/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3038 - g_loss: -0.9774\n",
      "Epoch 547/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2428 - g_loss: -0.8309\n",
      "Epoch 548/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1613 - g_loss: -0.8528\n",
      "Epoch 549/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2544 - g_loss: -0.9397\n",
      "Epoch 550/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2066 - g_loss: -0.7387\n",
      "Epoch 551/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2741 - g_loss: -0.8212\n",
      "Epoch 552/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2478 - g_loss: -0.8270\n",
      "Epoch 553/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2902 - g_loss: -0.9799\n",
      "Epoch 554/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1948 - g_loss: -0.8141\n",
      "Epoch 555/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1927 - g_loss: -0.8478\n",
      "Epoch 556/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2014 - g_loss: -0.9468\n",
      "Epoch 557/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2259 - g_loss: -0.9456\n",
      "Epoch 558/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1517 - g_loss: -1.0306\n",
      "Epoch 559/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1737 - g_loss: -1.0107\n",
      "Epoch 560/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -2.2121 - g_loss: -0.9980\n",
      "Epoch 561/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1848 - g_loss: -0.8979\n",
      "Epoch 562/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2816 - g_loss: -1.0085\n",
      "Epoch 563/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1883 - g_loss: -0.9450\n",
      "Epoch 564/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2576 - g_loss: -1.0055\n",
      "Epoch 565/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2163 - g_loss: -1.0192\n",
      "Epoch 566/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3261 - g_loss: -0.9224\n",
      "Epoch 567/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2229 - g_loss: -1.0836\n",
      "Epoch 568/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2157 - g_loss: -1.0758\n",
      "Epoch 569/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1403 - g_loss: -0.9516\n",
      "Epoch 570/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1124 - g_loss: -1.0827\n",
      "Epoch 571/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1682 - g_loss: -1.0713\n",
      "Epoch 572/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1802 - g_loss: -0.9079\n",
      "Epoch 573/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1936 - g_loss: -1.0242\n",
      "Epoch 574/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2196 - g_loss: -0.8627\n",
      "Epoch 575/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2790 - g_loss: -0.9487\n",
      "Epoch 576/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2150 - g_loss: -0.9419\n",
      "Epoch 577/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1715 - g_loss: -1.0676\n",
      "Epoch 578/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2344 - g_loss: -1.0655\n",
      "Epoch 579/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2310 - g_loss: -0.8150\n",
      "Epoch 580/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -2.2644 - g_loss: -0.9726\n",
      "Epoch 581/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2351 - g_loss: -0.9618\n",
      "Epoch 582/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2712 - g_loss: -0.9653\n",
      "Epoch 583/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2725 - g_loss: -1.0045\n",
      "Epoch 584/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1990 - g_loss: -0.9577\n",
      "Epoch 585/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2328 - g_loss: -1.1621\n",
      "Epoch 586/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2261 - g_loss: -0.8872\n",
      "Epoch 587/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2611 - g_loss: -1.1273\n",
      "Epoch 588/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3930 - g_loss: -0.9399\n",
      "Epoch 589/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2505 - g_loss: -1.0254\n",
      "Epoch 590/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2935 - g_loss: -1.0880\n",
      "Epoch 591/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2401 - g_loss: -1.0742\n",
      "Epoch 592/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2346 - g_loss: -0.9228\n",
      "Epoch 593/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2023 - g_loss: -0.9714\n",
      "Epoch 594/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2356 - g_loss: -0.9898\n",
      "Epoch 595/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1898 - g_loss: -1.0347\n",
      "Epoch 596/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2504 - g_loss: -0.9684\n",
      "Epoch 597/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2304 - g_loss: -0.7932\n",
      "Epoch 598/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2507 - g_loss: -0.8614\n",
      "Epoch 599/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2530 - g_loss: -0.8096\n",
      "Epoch 600/800\n",
      "42/42 [==============================] - 31s 731ms/step - d_loss: -2.1958 - g_loss: -0.7744\n",
      "Epoch 601/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3008 - g_loss: -0.7927\n",
      "Epoch 602/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2179 - g_loss: -0.7992\n",
      "Epoch 603/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1815 - g_loss: -0.8708\n",
      "Epoch 604/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1959 - g_loss: -0.7389\n",
      "Epoch 605/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2263 - g_loss: -0.9552\n",
      "Epoch 606/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2396 - g_loss: -0.9160\n",
      "Epoch 607/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2188 - g_loss: -0.8198\n",
      "Epoch 608/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2419 - g_loss: -0.8725\n",
      "Epoch 609/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1190 - g_loss: -0.8515\n",
      "Epoch 610/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1910 - g_loss: -0.9617\n",
      "Epoch 611/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2752 - g_loss: -0.9184\n",
      "Epoch 612/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2619 - g_loss: -0.9147\n",
      "Epoch 613/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2465 - g_loss: -1.0298\n",
      "Epoch 614/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1873 - g_loss: -0.9344\n",
      "Epoch 615/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2177 - g_loss: -1.0334\n",
      "Epoch 616/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1536 - g_loss: -0.9057\n",
      "Epoch 617/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2064 - g_loss: -0.9344\n",
      "Epoch 618/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1391 - g_loss: -0.8942\n",
      "Epoch 619/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2149 - g_loss: -1.0502\n",
      "Epoch 620/800\n",
      "42/42 [==============================] - 31s 728ms/step - d_loss: -2.1468 - g_loss: -0.8787\n",
      "Epoch 621/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2700 - g_loss: -0.8564\n",
      "Epoch 622/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2108 - g_loss: -0.8930\n",
      "Epoch 623/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2231 - g_loss: -0.9949\n",
      "Epoch 624/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2213 - g_loss: -0.9444\n",
      "Epoch 625/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1534 - g_loss: -0.9957\n",
      "Epoch 626/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2845 - g_loss: -1.0103\n",
      "Epoch 627/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2298 - g_loss: -0.9263\n",
      "Epoch 628/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2269 - g_loss: -1.1109\n",
      "Epoch 629/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2842 - g_loss: -1.0241\n",
      "Epoch 630/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2246 - g_loss: -0.9564\n",
      "Epoch 631/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2558 - g_loss: -0.8094\n",
      "Epoch 632/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2743 - g_loss: -0.9546\n",
      "Epoch 633/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2847 - g_loss: -0.8348\n",
      "Epoch 634/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2023 - g_loss: -0.9144\n",
      "Epoch 635/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2137 - g_loss: -0.8837\n",
      "Epoch 636/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2490 - g_loss: -1.0101\n",
      "Epoch 637/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1786 - g_loss: -0.9835\n",
      "Epoch 638/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1694 - g_loss: -0.9568\n",
      "Epoch 639/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1597 - g_loss: -0.7760\n",
      "Epoch 640/800\n",
      "42/42 [==============================] - 31s 730ms/step - d_loss: -2.1664 - g_loss: -0.8643\n",
      "Epoch 641/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2219 - g_loss: -1.0105\n",
      "Epoch 642/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2854 - g_loss: -0.9566\n",
      "Epoch 643/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.1799 - g_loss: -0.9798\n",
      "Epoch 644/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2076 - g_loss: -0.7827\n",
      "Epoch 645/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.1886 - g_loss: -0.8447\n",
      "Epoch 646/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2421 - g_loss: -0.8982\n",
      "Epoch 647/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2154 - g_loss: -0.8839\n",
      "Epoch 648/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3341 - g_loss: -0.9258\n",
      "Epoch 649/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2555 - g_loss: -0.8461\n",
      "Epoch 650/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2687 - g_loss: -0.9932\n",
      "Epoch 651/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2311 - g_loss: -0.8681\n",
      "Epoch 652/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2259 - g_loss: -1.0510\n",
      "Epoch 653/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1667 - g_loss: -0.9798\n",
      "Epoch 654/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2042 - g_loss: -0.9728\n",
      "Epoch 655/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2379 - g_loss: -1.0502\n",
      "Epoch 656/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2924 - g_loss: -0.9174\n",
      "Epoch 657/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2272 - g_loss: -0.8774\n",
      "Epoch 658/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2530 - g_loss: -0.8119\n",
      "Epoch 659/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2213 - g_loss: -1.0596\n",
      "Epoch 660/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -2.2322 - g_loss: -0.9981\n",
      "Epoch 661/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2239 - g_loss: -0.9661\n",
      "Epoch 662/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2001 - g_loss: -0.8621\n",
      "Epoch 663/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1795 - g_loss: -0.8943\n",
      "Epoch 664/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2163 - g_loss: -0.9248\n",
      "Epoch 665/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2561 - g_loss: -0.9336\n",
      "Epoch 666/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1956 - g_loss: -0.9368\n",
      "Epoch 667/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1935 - g_loss: -0.7674\n",
      "Epoch 668/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1883 - g_loss: -0.9197\n",
      "Epoch 669/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2629 - g_loss: -0.8852\n",
      "Epoch 670/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2193 - g_loss: -0.7010\n",
      "Epoch 671/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2371 - g_loss: -0.8575\n",
      "Epoch 672/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2195 - g_loss: -0.9768\n",
      "Epoch 673/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.3256 - g_loss: -0.8820\n",
      "Epoch 674/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2715 - g_loss: -0.9086\n",
      "Epoch 675/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2611 - g_loss: -0.7050\n",
      "Epoch 676/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1607 - g_loss: -0.8150\n",
      "Epoch 677/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2130 - g_loss: -0.9907\n",
      "Epoch 678/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2171 - g_loss: -1.0189\n",
      "Epoch 679/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2576 - g_loss: -1.0090\n",
      "Epoch 680/800\n",
      "42/42 [==============================] - 31s 730ms/step - d_loss: -2.2914 - g_loss: -1.0206\n",
      "Epoch 681/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.3222 - g_loss: -1.1375\n",
      "Epoch 682/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2430 - g_loss: -1.0465\n",
      "Epoch 683/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2129 - g_loss: -1.1035\n",
      "Epoch 684/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1686 - g_loss: -1.0200\n",
      "Epoch 685/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1725 - g_loss: -0.8884\n",
      "Epoch 686/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3637 - g_loss: -0.9052\n",
      "Epoch 687/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1853 - g_loss: -0.8837\n",
      "Epoch 688/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2375 - g_loss: -1.0433\n",
      "Epoch 689/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1695 - g_loss: -0.8844\n",
      "Epoch 690/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1779 - g_loss: -1.0417\n",
      "Epoch 691/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2355 - g_loss: -0.9614\n",
      "Epoch 692/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1436 - g_loss: -0.8576\n",
      "Epoch 693/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2234 - g_loss: -1.0812\n",
      "Epoch 694/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1120 - g_loss: -1.0709\n",
      "Epoch 695/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.1547 - g_loss: -1.0362\n",
      "Epoch 696/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2472 - g_loss: -1.0041\n",
      "Epoch 697/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2408 - g_loss: -1.1315\n",
      "Epoch 698/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2362 - g_loss: -0.8541\n",
      "Epoch 699/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1935 - g_loss: -1.0370\n",
      "Epoch 700/800\n",
      "42/42 [==============================] - 31s 730ms/step - d_loss: -2.2539 - g_loss: -1.0634\n",
      "Epoch 701/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1899 - g_loss: -1.1282\n",
      "Epoch 702/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2857 - g_loss: -0.9669\n",
      "Epoch 703/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1609 - g_loss: -1.1275\n",
      "Epoch 704/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2487 - g_loss: -1.1843\n",
      "Epoch 705/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2790 - g_loss: -0.9687\n",
      "Epoch 706/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2540 - g_loss: -1.1503\n",
      "Epoch 707/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2648 - g_loss: -0.9689\n",
      "Epoch 708/800\n",
      "42/42 [==============================] - 30s 715ms/step - d_loss: -2.2709 - g_loss: -1.0371\n",
      "Epoch 709/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2857 - g_loss: -1.0041\n",
      "Epoch 710/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2213 - g_loss: -0.9529\n",
      "Epoch 711/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2706 - g_loss: -0.8930\n",
      "Epoch 712/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2318 - g_loss: -0.9227\n",
      "Epoch 713/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1489 - g_loss: -0.9019\n",
      "Epoch 714/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2060 - g_loss: -0.9851\n",
      "Epoch 715/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2342 - g_loss: -1.0069\n",
      "Epoch 716/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2511 - g_loss: -1.0517\n",
      "Epoch 717/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2905 - g_loss: -0.9885\n",
      "Epoch 718/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2516 - g_loss: -0.9263\n",
      "Epoch 719/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2390 - g_loss: -0.9229\n",
      "Epoch 720/800\n",
      "42/42 [==============================] - 31s 730ms/step - d_loss: -2.3175 - g_loss: -0.8838\n",
      "Epoch 721/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2265 - g_loss: -0.9377\n",
      "Epoch 722/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2837 - g_loss: -1.0306\n",
      "Epoch 723/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1972 - g_loss: -1.0571\n",
      "Epoch 724/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2919 - g_loss: -0.9410\n",
      "Epoch 725/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2885 - g_loss: -0.8991\n",
      "Epoch 726/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1994 - g_loss: -0.9721\n",
      "Epoch 727/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2892 - g_loss: -0.8883\n",
      "Epoch 728/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2237 - g_loss: -0.9141\n",
      "Epoch 729/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1816 - g_loss: -0.9181\n",
      "Epoch 730/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2408 - g_loss: -0.9191\n",
      "Epoch 731/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2987 - g_loss: -0.9031\n",
      "Epoch 732/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2081 - g_loss: -0.9555\n",
      "Epoch 733/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2546 - g_loss: -0.9089\n",
      "Epoch 734/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2373 - g_loss: -0.9020\n",
      "Epoch 735/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1751 - g_loss: -0.8629\n",
      "Epoch 736/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2256 - g_loss: -0.8343\n",
      "Epoch 737/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2364 - g_loss: -0.9131\n",
      "Epoch 738/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1838 - g_loss: -1.0407\n",
      "Epoch 739/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1656 - g_loss: -1.0074\n",
      "Epoch 740/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -2.2523 - g_loss: -1.0811\n",
      "Epoch 741/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2262 - g_loss: -0.8260\n",
      "Epoch 742/800\n",
      "42/42 [==============================] - 30s 713ms/step - d_loss: -2.1915 - g_loss: -0.9930\n",
      "Epoch 743/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2570 - g_loss: -1.0563\n",
      "Epoch 744/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3048 - g_loss: -1.0527\n",
      "Epoch 745/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2657 - g_loss: -1.0180\n",
      "Epoch 746/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2248 - g_loss: -0.8929\n",
      "Epoch 747/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2038 - g_loss: -0.9075\n",
      "Epoch 748/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2221 - g_loss: -0.9686\n",
      "Epoch 749/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1822 - g_loss: -1.0119\n",
      "Epoch 750/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2217 - g_loss: -0.9923\n",
      "Epoch 751/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1145 - g_loss: -1.0014\n",
      "Epoch 752/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1823 - g_loss: -1.0354\n",
      "Epoch 753/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2004 - g_loss: -0.9609\n",
      "Epoch 754/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2151 - g_loss: -1.0291\n",
      "Epoch 755/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2655 - g_loss: -1.0196\n",
      "Epoch 756/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2038 - g_loss: -0.9368\n",
      "Epoch 757/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1594 - g_loss: -0.9077\n",
      "Epoch 758/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2421 - g_loss: -0.8918\n",
      "Epoch 759/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2657 - g_loss: -0.8866\n",
      "Epoch 760/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -2.2077 - g_loss: -0.8970\n",
      "Epoch 761/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2317 - g_loss: -0.9641\n",
      "Epoch 762/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1726 - g_loss: -0.9348\n",
      "Epoch 763/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2741 - g_loss: -0.8658\n",
      "Epoch 764/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2625 - g_loss: -0.8674\n",
      "Epoch 765/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1959 - g_loss: -0.9200\n",
      "Epoch 766/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1691 - g_loss: -0.8675\n",
      "Epoch 767/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1808 - g_loss: -0.9226\n",
      "Epoch 768/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1095 - g_loss: -0.9178\n",
      "Epoch 769/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2565 - g_loss: -0.8891\n",
      "Epoch 770/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2281 - g_loss: -1.0320\n",
      "Epoch 771/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1991 - g_loss: -0.8457\n",
      "Epoch 772/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2413 - g_loss: -0.8917\n",
      "Epoch 773/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1495 - g_loss: -1.0419\n",
      "Epoch 774/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2521 - g_loss: -0.9512\n",
      "Epoch 775/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2711 - g_loss: -0.8736\n",
      "Epoch 776/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1485 - g_loss: -0.9756\n",
      "Epoch 777/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2572 - g_loss: -0.8761\n",
      "Epoch 778/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1652 - g_loss: -0.9650\n",
      "Epoch 779/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2063 - g_loss: -0.7763\n",
      "Epoch 780/800\n",
      "42/42 [==============================] - 31s 729ms/step - d_loss: -2.2146 - g_loss: -0.9133\n",
      "Epoch 781/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3077 - g_loss: -0.8871\n",
      "Epoch 782/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2794 - g_loss: -0.8811\n",
      "Epoch 783/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2305 - g_loss: -0.9562\n",
      "Epoch 784/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2057 - g_loss: -0.8740\n",
      "Epoch 785/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1785 - g_loss: -0.9821\n",
      "Epoch 786/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3034 - g_loss: -0.9632\n",
      "Epoch 787/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1273 - g_loss: -1.0236\n",
      "Epoch 788/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1741 - g_loss: -0.8974\n",
      "Epoch 789/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1874 - g_loss: -0.9221\n",
      "Epoch 790/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2930 - g_loss: -1.0322\n",
      "Epoch 791/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2059 - g_loss: -0.9104\n",
      "Epoch 792/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1494 - g_loss: -0.7981\n",
      "Epoch 793/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1831 - g_loss: -0.8714\n",
      "Epoch 794/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.2733 - g_loss: -0.9225\n",
      "Epoch 795/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1756 - g_loss: -0.8191\n",
      "Epoch 796/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1916 - g_loss: -0.8483\n",
      "Epoch 797/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.3012 - g_loss: -0.7476\n",
      "Epoch 798/800\n",
      "42/42 [==============================] - 30s 714ms/step - d_loss: -2.1827 - g_loss: -0.8577\n",
      "Epoch 799/800\n",
      "42/42 [==============================] - 30s 713ms/step - d_loss: -2.1894 - g_loss: -0.7803\n",
      "Epoch 800/800\n",
      "42/42 [==============================] - 31s 726ms/step - d_loss: -2.2140 - g_loss: -0.9041\n"
     ]
    }
   ],
   "source": [
    "# instantiating the model in the strategy scope creates the model on the TPU\n",
    "with strategy.scope():\n",
    "\n",
    "    lr_gen = 0.00005\n",
    "    lr_disc = 0.0002\n",
    "    beta1 = 0.5\n",
    "    gp = 10\n",
    "    dstep = 5\n",
    "\n",
    "    # Instantiate the optimizer for both networks\n",
    "    generator_optimizer = keras.optimizers.RMSprop(\n",
    "        learning_rate=lr_gen\n",
    "    )\n",
    "    discriminator_optimizer = keras.optimizers.RMSprop(\n",
    "        learning_rate=lr_disc\n",
    "    )\n",
    "\n",
    "    # Define the loss functions for the discriminator,\n",
    "    # which should be (fake_loss - real_loss).\n",
    "    # We will add the gradient penalty later to this loss function.\n",
    "    # tf.nn.compute_average_loss is required when using TPU.\n",
    "    # when using GPU, I used tf.reduce_mean instead. \n",
    "    \"\"\"\n",
    "    def discriminator_loss(real_img, fake_img):\n",
    "        real_loss = tf.reduce_mean(real_img)\n",
    "        fake_loss = tf.reduce_mean(fake_img)\n",
    "        return fake_loss - real_loss\n",
    "    \"\"\"\n",
    "\n",
    "    def discriminator_loss(real_img, fake_img):\n",
    "        real_loss = tf.nn.compute_average_loss(real_img, global_batch_size=128*8)\n",
    "        fake_loss = tf.nn.compute_average_loss(fake_img, global_batch_size=128*8)\n",
    "        return fake_loss - real_loss\n",
    "    \n",
    "    \"\"\"\n",
    "    # Define the loss functions for the generator.\n",
    "    def generator_loss(fake_img):\n",
    "        return -tf.reduce_mean(fake_img)\n",
    "    \"\"\"\n",
    "    \n",
    "    def generator_loss(fake_img):\n",
    "        return -tf.nn.compute_average_loss(fake_img, global_batch_size=128*8)\n",
    "    \n",
    "\n",
    "    # Set the number of epochs for trainining.\n",
    "    epochs = 2000\n",
    "\n",
    "    # Instantiate the customer `GANMonitor` Keras callback.\n",
    "    cbk = GANMonitor(num_img=3, latent_dim=100)\n",
    "\n",
    "\n",
    "    # Save the model after every epoch\n",
    "    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "    now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = \"./savedModels/\" + str(now) + \"_model/{epoch:02d}/\"\n",
    "    # If you want name to be static:\n",
    "    #filepath = \"./savedModels/xxx_model/{epoch:02d}/\"\n",
    "    savepoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=False, save_freq=840, options=save_locally)\n",
    "\n",
    "    # Instantiate the WGAN model.\n",
    "    wgan = WGAN(\n",
    "        discriminator=discriminator_model,\n",
    "        generator=generator_model,\n",
    "        latent_dim=100,\n",
    "        discriminator_extra_steps=dstep,\n",
    "        gp_weight=gp\n",
    "    )\n",
    "\n",
    "    # Compile the WGAN model.\n",
    "    wgan.compile(\n",
    "        d_optimizer=discriminator_optimizer,\n",
    "        g_optimizer=generator_optimizer,\n",
    "        g_loss_fn=generator_loss,\n",
    "        d_loss_fn=discriminator_loss,\n",
    "        steps_per_execution=16\n",
    "    )\n",
    "\n",
    "    # Uncomment to load a model\n",
    "    #load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
    "    #wgan.load_weights(dest, options = load_locally)\n",
    "\n",
    "# Start training the model.\n",
    "print(\"starting training\")\n",
    "history = wgan.fit(images[:42000], batch_size=128*8, epochs=epochs, initial_epoch=0, callbacks=[cbk, savepoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24235.874736,
   "end_time": "2021-06-14T07:48:44.060091",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-14T01:04:48.185355",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}